---
title: "R Notebook"
output: html_notebook
---

# Logistic Regression

## Main Ideas

```{r}
library(tidyverse)
```

How would we fit a line to this?
```{r}
# logistic_model <- glm(pass ~ time_spent_studying, family = binomial(), student_results)

student_results <- tibble(
  time_spent_studying = c(1.5, 1, 2.2, 1.2, 4.6, 5.0, 2.6, 2.5, 4, 3.2, 0.5, 0.8, 1.8, 4.5, 3.8, 2.8, 4.2, 3, 3.5, 0.2),
  pass = c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE)
)

ggplot(student_results, aes(time_spent_studying, pass)) +
  geom_point()
```

We could try fitting a straight line
```{r}
lm(pass ~ time_spent_studying,
   data = student_results)
```
Why doesn't linear regression work well for this data??

* Can predict outside of the range
* High error

Instead, a fit we might want:

* predict a probability of Pass / Fail (0 - 1)
* would closely align with data points

How do we make a line do this??

* logistic regression

First, we need to talk about `Odds`

* a ratio of success : failure
* odds of tossing a coin and getting a head are:
  + 1 success (head) : 1 failure (tails)
  + 1:1
* odds of rolling a 6 on a fair dice:
  + 1 success (6) : 5 failures (1, 2, 3, 4, 5)
  + 1:5


```{r}
mortgage <- read_csv("data/mortgage_applications.csv") %>% janitor::clean_names()
```


```{r}
library(GGally)
mortgage %>% 
  ggpairs()
```

```{r}
mortgage %>% 
  ggplot(aes(x = tu_score)) +
  geom_histogram()
```


```{r}
mortgage %>% 
  ggplot(aes(x = tu_score,
             y = accepted)) +
  geom_point()
```


```{r}
mortgage %>% 
  ggplot(aes(x = tu_score,
             y = as.integer(accepted))) +
  geom_jitter()
```


Logistic Regression Assumptions

* dependent variable should be binary
* no outliers in the data
* no high correlations between predictors
* can be used to predict probability of an event
* often used to classify

Glossary:

* probability
  + n success / n outcomes
  + 1 / 6 for rolling a 6
  + prob (x) = odds (x) / 1 + odds (x)
* odds
  + n success / n failures
  + 1 : 5 for rolling a 6
  + odds (x) = prob (x) / 1 - prob (x) 
  + odds (rolling success) = (1 / 6) / 1 - (1 / 6) = 0.2
* odds ratio 

* ln / log

* link funciton

* logit

* odds factor


```{r}
logit <- function(x){
  return(log(x / (1-x)))
}

logit_data <- tibble(p = seq(0.001, 0.999, 0.001)) %>% 
  mutate(logit_p = logit(p))

head(logit_data)
```


```{r}
logit_data %>% 
  ggplot(aes(x = logit_p,
             y = p)) +
  geom_line() + 
  labs(x = "logit (p) value",
       y = "probability")
```


## Logistic Regression in R

```{r}
mortgage_logreg_model <- glm(accepted ~ tu_score,
                             data = mortgage,
                             family = binomial(link = "logit"))

summary(mortgage_logreg_model)
```

p^ = 1 / (1 + exp(-4.575035 + 0.008475 tu_score))

**as tu_score increases, the probability of being accepted increases**
a 1 unit increase of tu_score increases the log odds by 0.008

```{r}
library(modelr)
```


```{r}
# 0 - 710 are the options for credit score
# map a prediction of accepted or not for credit score
log_predictions <- tibble(
  tu_score = seq(0, 710, 1)
) %>% 
  add_predictions(model = mortgage_logreg_model,
                  type = "response")

ggplot(mortgage)+
  geom_jitter(aes(x = tu_score,
                  y = as.numeric(accepted)), alpha = 0.5, height = 0.1, width = 0.1) +
  geom_line(data = log_predictions, aes(x = tu_score, y = pred), col = "red")+
  labs(y = "Estimated (Accepted)")
```

Use predictions to filter prediction table to look at odds
```{r}
log_predictions %>% 
  filter(tu_score == 600)

log_predictions %>% 
  filter(tu_score == 300)
```
How do our odds of getting accepted for a mortgage change as we increase tu_score??

odds at a baseline level (tu_score = 594)
odds at a bit above baseline level(tu_score + 50)

how does increasing our tu_score by 50 affect our odds??

```{r}
odds_change <- function(b1, change){
  exp(b1 * change)
}

odds_change(0.008475, 50)
```
a 50 unit increase of tu_score increases our odds by a factor of 1.52768

## Categorical Predictors

```{r}
mortgage_model_2_terms <- glm(
  accepted ~ tu_score + employed,
  data = mortgage,
  family = binomial(link = "logit")
)

summary(mortgage_model_2_terms)
```

ln(Odds(Accepted)) = b0 + b1 * tu_score + b2 * employed_TRUE
ln(Odds(Accepted)) = b0 + 0.0067239 * tu_score + 1.4845379 * employed_TRUE


```{r}
# map a prediction of accepted or not for each credit score for employment
log_predictions <- tibble(
  tu_score = rep(seq(0, 710, 1), 2),
  employed = c(rep(TRUE, 711), rep(FALSE, 711))
) %>% 
  add_predictions(model = mortgage_model_2_terms,
                  type = "response")

ggplot(mortgage)+
  geom_jitter(aes(x = tu_score,
                  y = as.numeric(accepted)), alpha = 0.5, height = 0.1, width = 0.1) +
  geom_line(data = log_predictions, aes(x = tu_score, y = pred, colour = employed))+
  labs(y = "Estimated (Accepted)")
```


```{r}
log_predictions %>% 
  filter(tu_score == 600)

log_predictions %>% 
  filter(tu_score == 300)
```


Interpretation of categorical predictors:

Again we look at compared to our reference level

odds(employed = TRUE) / odds(employed = FALSE) = odds ratio

```{r}
odds_change(1.48, 1)
```
On average, the customer's odds of being accepted for a mortgage are 4.39 times higher if they are employed


## Model Performance: Building a Binary Classifier

Rather than knowing the equation of a squiggle and how increasing/decreasing your credit score / employment affected the probability you'd be accepted for a mortgage...

a model that assigns a class based on this predicted probability

* if applicant approaches our bank asking for a mortgage:
  + if their credit score is X and employment is Y, will they be accepted?
  
The probability level at which we assign "yes" and "no" is called the **threshold**

so, for a 0.6 threshold, if the predicted probability of being accepted is greater than 0.6 --> accept

```{r}
mortgage_preds_2_terms <- mortgage %>% 
  # add our predicted probs from the log model
  add_predictions(mortgage_model_2_terms, type = "response") %>% 
  # this is the binary classifier step
  mutate(pred_accepted = pred >= 0.6)
```

## Model Assessment - How often did we get it right?

* Quick Assessment Tool: The Confusion Matrix

```{r}
library(janitor)
```

```{r}
mortgage_preds_2_terms %>% 
  tabyl(accepted, pred_accepted)
```
          | predictions |
| reality |   TN
                    TP


```{r}
mortgage_3pred_model <- glm(accepted ~ tu_score + age + employed,
                            data = mortgage,
                            family = binomial(link = "logit"))

summary(mortgage_3pred_model)
```


```{r}
mortage_data_with_3pred <- mortgage %>% 
  add_predictions(mortgage_3pred_model, type = "response") %>% 
  mutate(pred_accepted = pred >= 0.6)
```

```{r}
mortage_data_with_3pred %>% 
  tabyl(accepted, pred_accepted) %>% 
  adorn_title()
```

You will always have:

* times your model was **correct** (FALSE:FALSE (679) & TRUE:TRUE (179))
* times your model was **wrong** (FALSE:TRUE (93) & TRUE:FALSE(49))
* 0's in the **incorrect** diagonal would be indicative of a perfect classifier
* TN - FALSE:FALSE (People didn't get a mortgage, model predicts no mortgage)
* TP - TRUE:TRUE (People did get a mortgage, model predicts mortgage)
* FP - TRUE:FALSE (People didn't get a mortgage, model predicts mortgage)
* FN - FALSE:TRUE (People did get a mortgage, model predicts no mortgage)


Task - 5 mins
Extract the rows in mortgage_data with tu_score = 594. Compare the sample data outcomes with the predicted outcomes of the threshold 0.6 classifier and say which of the following four groups each outcome belongs to: (i) true positive, (ii) true negative, (iii) false positive or (iv) false negative.
```{r}
mortage_data_with_3pred %>% 
  filter(tu_score == 594)
```

i. FALSE POSITIVE
ii. FALSE POSITIVE
iii. TRUE POSITIVE
iv. TRUE POSITIVE
v. TRUE POSITIVE

What measures of accuracy do we have?
How do we boil this down into a single value of accuracy

Accuracy = nTP + nTN / N
(times we were right) / (nrows)

```{r}
mortage_data_with_3pred %>% 
  tabyl(accepted, pred_accepted) %>% 
  adorn_title()

(679 + 179) / (nrow(mortage_data_with_3pred))
```
We were correct in our predictions about 85% of the time

The bad news is that accuracy has a subtle weakness

Consider this case:

We run a test at the end of the school year for 1000 students
900 students pass the test
100 students do not pass the test

what accuracy would my 'always pass' model get?
900 / 1000 = 90%

the subtle weakness of accuracy is that if the model is unbalanced, it can show the model to be highly accurate 

This is why we've got other performance measures:

* accuracy 
* rates
* specificity
* AUC
* GINI

True Positive Rate / Sensitivity

The probability that an actual positive will test positive

TP / TP + FN

```{r}
mortage_data_with_3pred %>% 
  tabyl(accepted, pred_accepted) %>% 
  adorn_title()

nTN = 679
nFP = 49
nFN = 93
nTP = 179

true_positive_rate <- nTP / (nTP + nFN)
true_positive_rate

true_negative_rate <- nTN / (nTN + nFP)
true_negative_rate

false_positive_rate <- nFP / (nFP + nTN)
false_positive_rate

false_negative_rate <- nFN / (nFN + nTP)
false_negative_rate
```

Which should we prioritise??

* no **one** answer to this
* a loan-giving business might not mind as many false positives if it means they still give lots of people loans that should get them
* a disease testing company would care **a lot** about false positives




